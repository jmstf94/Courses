{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel methods are widespread in machine learning and they were particularly common before deep learning became a dominant paradigm. The core idea is to introduce a new notion of distance between high-dimensional data points by replacing the inner product $(x_i, x_j)$ by a function that retains many properties of the inner product, yet which is nonlinear. This function $k(.,.)$ is called a kernel. Then, in many cases, wherever a learning algorithm would use an inner product, the kernel function is used instead.\n",
    "\n",
    "The intuition is that the kernel function acts as an inner product on a higher dimensional space and encompasses some $\\phi(.)$ mapping from the original space of the data points to this space. So intuitively, the kernel function is $k(x_i, x_j)=(\\phi(x_i), \\phi(x_j))$. The hope is that points that were not linearly separable in the original space become linearly separable in the higher dimensional space. The $\\phi(.)$ function may map to an infinite dimensional space and it does not actually have to be specified. As long as the kernel function is positive semidefinite, the idea works.\n",
    "\n",
    "Many kernel-based learning algorithms are instance-based, which means that the final model retains some or all of the training instances and they play a role in the actual prediction. Support vector machines belong here: support vectors are the training instances which are critically important in defining the boundary between two classes. Some important kernels are listed below.\n",
    "\n",
    "| Name | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kernel function|\n",
    "|------|-----------------|\n",
    "|Linear | $(x_i,x_j)$|\n",
    "|Polynomial| $((x_i,x_j)+c)^d$|\n",
    "|Radial basis function|$\\exp(-\\gamma\\|x_i-x_j\\|^2)$|\n",
    "\n",
    "The choice of kernel and the parameters of the kernel are often arbitrary and either some trial and error on the dataset or hyperparameter optimization helps choose the right combination. Quantum computers naturally give rise to certain kernels and it is worth looking at a specific variant of how it is constructed.\n",
    "\n",
    " # Thinking backward: learning methods based on what the hardware can do\n",
    "\n",
    "Instead of twisting a machine learning algorithm until it only contains subroutines that have quantum variants, we can reverse our thinking and ask: given a piece of quantum hardware and its constraints, can we come up with a new learning method? For instance, interference is a very natural thing to do: we showed an option in the first notebook on quantum states, and it can also be done with a Hadamard gate. \n",
    "For this to work we need to encode both training and testvectors as amplitudes in a statevector built up out of four registers:\n",
    "\n",
    "$|0\\rangle_c|00..0\\rangle_m|00..0\\rangle_i|0\\rangle_a$\n",
    "\n",
    "The amplitude of such state will be equal to the value of a feature in a training vector or test vector. To do that we use four registers. The first is a single bit, acting as the ancilla ancilla (a), which will will code for either a training (a=0) or a testvector (a=1). The second register, in the notebook example a single bit, will code for the m-th training vector. The third register, in the notebook example also reduced to a single bit, codes for the i-th feature. Lastly the class bit (c) codes for class -1 (c=0), or 1 (c=1).\n",
    "Hence, if after fully encoding all training data and test data into the state $|\\psi>$ the state |1010> has coefficient 0.46 :\n",
    "\n",
    "$|\\psi\\rangle\\ = ....+ 0.46|1010\\rangle +....$  ,\n",
    "\n",
    "Then that implies that the second feature (i=1) of the first (m=0) training vector (a=0), which classifies as class 1 (c=1), has the value 0.46. Note, we assume both training vectors and test vector are normalized.\n",
    "\n",
    "In a more general expression we can write for a fully encoded state (NB we arrange the order of the registers to be consistent with the code below):\n",
    "\n",
    "$|\\psi\\rangle = \\frac{1}{\\sqrt{2M}}\\sum_{m=0}^{M-1}|y_m\\rangle|m\\rangle|\\psi_{x^m}\\rangle|0\\rangle + |y_m\\rangle|m\\rangle|\\psi_{\\tilde{x}}\\rangle|1\\rangle$\n",
    "\n",
    "with:\n",
    "\n",
    "$|\\psi_{x^m}\\rangle = \\sum_{i=0}^{N-1}x_i^m|i\\rangle, \\; |\\psi_{\\tilde{x}}\\rangle = \\sum_{i=0}^{N-1}\\tilde{x_i}|i\\rangle. \\quad$ N being equal to the number of features in the the training and test vectors\n",
    "\n",
    "As the last summation is independent on m, there will M copies of the test vector in the statevector, one for every training vector.\n",
    "\n",
    "\n",
    "We now only need to apply a Hadamard gate to the ancilla to interfere the test and training instances. Measuring and post-selecting on the ancilla gives rise to a kernel [[1](#1)].\n",
    "\n",
    "Let's start with initializations:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:52.516626Z",
     "start_time": "2019-02-01T23:26:51.787904Z"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit import ClassicalRegister, QuantumRegister, QuantumCircuit\n",
    "from qiskit import execute\n",
    "from qiskit import Aer\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "\n",
    "q = QuantumRegister(4)\n",
    "c = ClassicalRegister(4)\n",
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are constructing an instance-based classifier: we will calculate a kernel between all training instances and a test example. In this sense, this learning algorithm is lazy: no actual learning happens and each prediction includes the entire training set.\n",
    "\n",
    "As a consequence, state preparation is critical to this protocol. We have to encode the training instances in a superposition in a register, and the test instances in another register. Consider the following training instances of the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris): $S = \\{(\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}, 0), (\\begin{bmatrix}0.790 \\\\ 0.615\\end{bmatrix}, 1)\\}$, that is, one example from class 0 and one example from class 1. Furthermore, let's have two test instances, $\\{\\begin{bmatrix}-0.549\\\\ 0.836\\end{bmatrix}, \\begin{bmatrix}0.053 \\\\ 0.999\\end{bmatrix}\\}$. These examples were cherry-picked because they are relatively straightforward to prepare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:52.525627Z",
     "start_time": "2019-02-01T23:26:52.518665Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set = [[0, 1], [0.79, 0.615]]\n",
    "labels = [0, 1]\n",
    "test_set = [[-0.549, 0.836], [0.053 , 0.999]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data vectors, we use amplitude encoding as explained above, which means that, for instance, the second training vector will be encoded as $0.78861006|0\\rangle + 0.61489363|1\\rangle$. Preparing these vectors only needs a rotation, and we only need to specify the corresponding angles. The first element of the training set does not even need that: it is just the $|1\\rangle$ state, so we don't specify an angle for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the angle we need to solve the equation $a|0\\rangle + b|1\\rangle=\\cos\\left(\\frac{\\theta}{2}\\right)|0\\rangle + i \\sin \\left(\\frac{\\theta}{2}\\right) |1\\rangle$. Therefore, we will use $\\theta=2 \\arccos(a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(amplitude_0):\n",
    "    return 2*np.arccos(amplitude_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the state preparation procedure we will consider requires the application of several rotations in order to prepare each data point in the good register. Don't hesitate to check it by yourself by running the circuit below with a pen and paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function builds the circuit. We plot it and explain it in more details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:52.589616Z",
     "start_time": "2019-02-01T23:26:52.548747Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_state(q, c, angles):\n",
    "    ancilla_qubit = q[0]\n",
    "    index_qubit = q[1]\n",
    "    data_qubit = q[2]\n",
    "    class_qubit = q[3]\n",
    "    circuit = QuantumCircuit(q, c)\n",
    "    # Put the ancilla and the index qubits into uniform superposition\n",
    "    circuit.h(ancilla_qubit)\n",
    "    circuit.h(index_qubit)\n",
    "    circuit.barrier()\n",
    "\n",
    "    # Prepare the test vector\n",
    "    circuit.cu3(angles[0], 0, 0, ancilla_qubit, data_qubit)\n",
    "    # Flip the ancilla qubit > this moves the input \n",
    "    # vector to the |0> state of the ancilla\n",
    "    circuit.x(ancilla_qubit)\n",
    "    circuit.barrier()\n",
    "\n",
    "\n",
    "    # Prepare the first training vector\n",
    "    # [0,1] -> class 0\n",
    "    # We can prepare this with a Toffoli\n",
    "    circuit.ccx(ancilla_qubit, index_qubit, data_qubit)\n",
    "    # Flip the index qubit > moves the first training vector to the \n",
    "    # |0> state of the index qubit\n",
    "    circuit.x(index_qubit)\n",
    "    circuit.barrier()\n",
    "\n",
    "    # Prepare the second training vector\n",
    "    # [0.790, 0.615] -> class 1\n",
    "    #\n",
    "    # Ideally we would do this with a double controlled, i.e a ccry, gate\n",
    "    # However in qiskit we cannot build such a gate, hence we resort to\n",
    "    # the following construction\n",
    "    \n",
    "    circuit.ccx(ancilla_qubit, index_qubit, data_qubit)\n",
    "    circuit.ry(-angles[1], data_qubit)\n",
    "    circuit.ccx(ancilla_qubit, index_qubit, data_qubit)\n",
    "    circuit.ry(angles[1], data_qubit)\n",
    "    circuit.barrier()\n",
    "\n",
    "    # Flip the class label for training vector #2\n",
    "    circuit.cx(index_qubit, class_qubit)\n",
    "   \n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the circuit for the distance-based classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:53.878539Z",
     "start_time": "2019-02-01T23:26:52.591175Z"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit.tools.visualization import circuit_drawer\n",
    "\n",
    "# Compute the angles for the testvectors\n",
    "test_angles = [get_angle(test_set[0][0]), get_angle(test_set[1][0])]\n",
    "# Compute the angle for the second training vector (as the first vector is trivial)\n",
    "training_angle = get_angle(training_set[1][0])/2\n",
    "\n",
    "angles = [test_angles[0], training_angle]\n",
    "state_preparation_0 = prepare_state(q, c, angles)\n",
    "circuit_drawer(state_preparation_0, output='mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical lines are barriers to make sure that all gates are finished by that point. They also make a natural segmentation of the state preparation.\n",
    "\n",
    "In the first section, the ancilla and index qubits are put into uniform superposition.\n",
    "\n",
    "The second section entangles the test vector with the ground state of the ancilla.\n",
    "\n",
    "In the third section, we prepare the state $|1\\rangle$, which is the first training instance, and entangle it with the excited state of the ancilla and the ground state of the index qubit with a Toffoli gate and a Pauli-X gate. The Toffoli gate is also called the controlled-controlled-not gate, describing its action.\n",
    "\n",
    "The fourth section prepares the second training instance and entangles it with the excited state of the ancilla and the index qubit. Next, the class qubit is flipped conditioned on the index qubit being $|1\\rangle$. This creates the connection between the encoded training instances and the corresponding class label.\n",
    "\n",
    "Let's dissect the last part where we prepare the second training state, which is $\\begin{pmatrix}0.790 \\\\ 0.615\\end{pmatrix}$ and we entangle it with the excited state of the ancilla and the excited state of the index qubit. We use `angles[1]`, which is ~`1.325/2`. Why? We have to rotate the basis state $|0\\rangle$ to contain the vector we want. We could write this generic state as $\\begin{pmatrix} \\cos(\\theta/2) \\\\ \\sin(\\theta/2)\\end{pmatrix}$. Looking at the documentation of the gate implementing the rotation, you'll see that the function argument divides the angle by two, so we have to adjust for that -- this is why we divided $\\theta$ by two. If you calculate the arccos or arcsin values, you will get the value in `angles[1]`.\n",
    "\n",
    "\n",
    "\n",
    "We need to apply the rotation to data qubit only if ancilla AND index qubits are 1, in other words, we have to implement a double controlled rotation. Qiskit does not have this type of gate. Hence, we'll build it in two stages of half the required angle, designed in such a way they either add or cancel. The quantum AND gate is the CCX (also known as Toffoli), which flips the target qubit if both controls are 1. Applying the CCX flips only the data qbit of the target state. The subsequent rotation over half the desired angle works on all states, but after applying the second CCX the targed state has actually rotated in the opposite direction. Applying the reverse rotation adds the second half of the rotation for the target state,ans cancels the rotation for all other states (See Bloch sphere diagram outlining these 4 steps)\n",
    "\n",
    "\n",
    "![Bloch sphere](../figures/blochsphere.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's now see what final state the circuit has produced. Note, the print is a non-normalized statevector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ['Xtest x', 'Xtrn0 x','','','Xtest y','Xtrn0 y','','','','','Xtest x','Xtrn1 x','','','Xtest y','Xtrn1 y']\n",
    "res = execute(state_preparation_0, Aer.get_backend('statevector_simulator')).result()\n",
    "outp = 2* np.array(np.real(res.get_statevector(state_preparation_0)))\n",
    "print('Statevector after insertion of data and testvectors\\n\\ncdia   coefficient')\n",
    "for z in range(outp.shape[0]):\n",
    "    print(format(z, '04b'),'    % 5.4f   ' %(round(outp[z],3)), val[z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table you can see how both the test vector (Xtst x, Xtsty), as well as the training vectors ((Xtrn0 x, Xtrn0 y) - class0) and ((Xtrn1 x,Xtrn1 y) - class1) are embedded in the state vector. The training vector class is indicated in the class bit (c). The test vector is coded by the 0-state of the ancilla (a), and the training vector is coded by the 1-state of the ancilla. Note also the data bit (d) coding for the value of the x or y feature of the training vectors, and the index bit (i) coding for training vector 1 or 2.\n",
    "\n",
    "We are now ready for the final step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A natural kernel on a shallow circuit\n",
    "\n",
    "Having done the state preparation, the actual prediction is nothing but a Hadamard gate applied on the ancilla, followed by measurements. Since the ancilla is in a uniform superposition at the end of the state preparation and it is entangled with the registers encoding the test and training instances, applying a second Hadamard on the ancilla interferes the entangled registers. The state before the measurement is  $\\frac{1}{2\\sqrt{M}}\\sum_{m=0}^{M-1}|y_m\\rangle|m\\rangle(|\\psi_{x^m}\\rangle+|\\psi_{\\tilde{x}}\\rangle)|0\\rangle+|y_m\\rangle|m\\rangle(|\\psi_{x^m}\\rangle-|\\psi_{\\tilde{x}}\\rangle)|1\\rangle$, where $|\\psi_{\\tilde{x}}\\rangle$ is the encoded test instance and $\\psi_{x^m}\\rangle$ is the m-th training instance. For our example M, the number of training samples, equals 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:53.884698Z",
     "start_time": "2019-02-01T23:26:53.880801Z"
    }
   },
   "outputs": [],
   "source": [
    "def interfere_data_and_test_instances(circuit, q, c, angles):\n",
    "    circuit.h(q[0])\n",
    "    circuit.barrier()\n",
    "    circuit.measure(q, c)\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we measure the ancilla, the outcome probability of observing 0 will be $\\frac{1}{4M}\\sum_{i=0}^{M-1} |\\tilde{x} + x_m|^2$. This creates a kernel of the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:54.007377Z",
     "start_time": "2019-02-01T23:26:53.886046Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "x = np.linspace(-2, 2, 100)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.plot(x, 1-x**2/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the kernel that performs the classification. We perform the post-selection on observing 0 on the measurement on the ancilla and calculate the probabilities of the test instance belonging to either class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:54.017909Z",
     "start_time": "2019-02-01T23:26:54.009283Z"
    }
   },
   "outputs": [],
   "source": [
    "def postselect(result_counts):\n",
    "    total_samples = sum(result_counts.values())\n",
    "\n",
    "    # define lambda function that retrieves only results where the ancilla is in the |0> state\n",
    "    post_select = lambda counts: [(state, occurences) for state, occurences in counts.items() if state[-1] == '0']\n",
    "\n",
    "    # perform the postselection\n",
    "    postselection = dict(post_select(result_counts))\n",
    "    postselected_samples = sum(postselection.values())\n",
    "    ancilla_post_selection = postselected_samples/total_samples\n",
    "\n",
    "    print('Ancilla post-selection probability was found to be ',round(ancilla_post_selection,3))\n",
    "    retrieve_class = lambda binary_class: [occurences for state, occurences in postselection.items() if state[0] == str(binary_class)]\n",
    "\n",
    "    prob_class0 = sum(retrieve_class(0))/postselected_samples\n",
    "    prob_class1 = sum(retrieve_class(1))/postselected_samples\n",
    "\n",
    "    print('Probability for class 0 is', round(prob_class0,3))\n",
    "    print('Probability for class 1 is', round(prob_class1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first instance we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:54.546426Z",
     "start_time": "2019-02-01T23:26:54.019350Z"
    }
   },
   "outputs": [],
   "source": [
    "circuit_0 = interfere_data_and_test_instances(state_preparation_0, q, c, angles)\n",
    "job = execute(circuit_0, backend)\n",
    "result = job.result()\n",
    "count = result.get_counts(circuit_0)\n",
    "print(count)\n",
    "postselect(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the second one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:26:54.627141Z",
     "start_time": "2019-02-01T23:26:54.548825Z"
    }
   },
   "outputs": [],
   "source": [
    "angles = [test_angles[1], training_angle]\n",
    "state_preparation_1 = prepare_state(q, c, angles)\n",
    "circuit_1 = interfere_data_and_test_instances(state_preparation_1, q, c, angles)\n",
    "job = execute(circuit_1, backend)\n",
    "result = job.result()\n",
    "count = result.get_counts(circuit_1)\n",
    "print(count)\n",
    "postselect(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] M. Schuld, M. Fingerhuth, F. Petruccione. (2017). [Implementing a distance-based classifier with a quantum interference circuit](https://doi.org/10.1209/0295-5075/119/60002). *Europhysics Letters*, 119(6), 60002. <a id='1'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
